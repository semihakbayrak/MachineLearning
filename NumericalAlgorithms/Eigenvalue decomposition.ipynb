{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For symmetric $A$, the critical points of $x^T A x$ subject to $||x||_2 = 1$ are eigenvectors of $A$:\n",
    "\n",
    "$C(x) = x^T A x - \\lambda (1-x^Tx)$\n",
    "\n",
    "$\\nabla_x C(x) = 0 => 2Ax - 2\\lambda x = 0 => Ax = \\lambda x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$ must be symmetric (or hermitian) for eigenvectors to be critical points, otherwise $\\nabla_x x^TAx = (A^T+A)x \\neq 2Ax$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraint $||x||_2=1$ is also required since it prevents critical point to be $x=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of objectives are ubiquitous. Some examples are given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 1 (PCA):__ Consider $N$ instances $x^{1:N}$ where each instance $x^i$ has $d$ features. All these data points are represented with $N \\times d$ dimensional matrix $X$ and the mean of the feautures and covariance between the features are $d$ and $d \\times d$ dimensional $\\mu$ and $\\Sigma$, respectively. Sometimes some of the features are highly correlated so not all the features are informative. We may then find a new lower dimensional ($k$ s.t. $k<d$) space where the data instances are scattered well and easily seperable. Let's get started with the first new dimension. Suppose a unit vector $q_1$ lies on this one-dimensional space. Then the projection of an instance $x^i$ on this space is $z_1^i = q_1^T x^i q_1$. Doing this projection to all instances we get $z_1^{i:N}$ with mean $m_1 = q_1^T \\mu$ and variance $v_1 = q_1^T \\Sigma q_1$. In order for these new instances to be scattered as much as possible, we want variance $v_1$ to be maximum. Therefore our task becomes: $\\underset{q_1}{\\text{max }} q_1^T \\Sigma q_1$ subject to $q_1^T q_1 = 1$. This objective is maximized with the largest eigenvalue of $\\Sigma$:\n",
    "\n",
    "$\\underset{q_1}{\\text{max }} q_1^T \\Sigma q_1 - \\lambda(1 - q_1^T q_1) => \\Sigma q_1 = \\lambda q_1$.\n",
    "\n",
    "We want second one-dimensional new space to maximize the variance, as well. We introduce unit variance $q_2$ which is supposed to be orthogonal to $q_1$: \n",
    "\n",
    "$\\underset{q_2}{\\text{max }} q_2^T \\Sigma q_2 - \\lambda_1(1 - q_2^T q_2) - \\lambda_2(0-q_2^T q1) => \\Sigma q_2 - \\lambda_1 q_2 - 0.5 \\lambda_2 q_1 = 0$\n",
    "\n",
    "Multiplying with $q_1^T$, the above equation turns into $\\underbrace{q_1^T \\Sigma q_2}_{\\lambda q_1^T q_2 = 0} - \\underbrace{\\lambda_1 q_1^T q_2}_{0} - 0.5 \\lambda_2 q_1^T q_1 = 0$\n",
    "\n",
    "So $\\lambda_2 = 0$ => $\\Sigma q_2 = \\lambda_1 q_2$ => $q_2$ is the second largest eigenvector. In a similar fashion, we can extend it to $k$ dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 2 (Differential equations):__ Many dynamical systems can be formulated in the form of $y' = B y$. Assume we know the $k$ eigenvectors of $B$ which are $y_1, y_2, ..., y_k$. Writing the initial condition by using these eigenvectors, we can make the analysis of the state of this dynamical system at time t:\n",
    "\n",
    "$\n",
    "y(0) = c_1 y_1 + c_2 y_2 + ... + c_k y_k \\\\\n",
    "y(t) = c_1 e^{\\lambda_1 t}y_1 + c_2 e^{\\lambda_2 t}y_2 + ... + c_k e^{\\lambda_k t}y_k\n",
    "$\n",
    "\n",
    "In many situations, it is expensive to find the eigenvectors and $B$ may change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 3 (Spectral Embedding):__ For a given collection of data instances (such as photos, documents, twitter acounts), we may want to locate them in a lower dimensional space such that similar instances will be close to each other. In one dimensional space, this task refers to locating the instances on numerical axis. Similiarity between instances $i$ and $j$ is scored with $w_{ij} = w_{ji} \\geq 0$. This score can be based on any reasonable similiarity metric (such as sth based on pixel values, frequency of words appear in documents, or a graph structure that indicates the connections between the accounts). Instances $i$ and $j$ takes the values $x_i$ and $x_j$ on the numerical axis and all the values are stored in the vector $x = [x_1, x_2, ... ]$. We introduce an energy function to minimize to achieve this setting: $E(x) = \\sum\\limits_{ij}w_{ij}(x_i-x_j)^2$. Note that this would be minimized by setting all numbers to zero or equal to each other. We avoid this circumstances by additional two constraints that are $||x||_2^2 = 1$ and $1_d^T x = 0$ where $1_d^T$ is $d$ dimensional vector of ones. So the overall objective is:\n",
    "\n",
    "$\n",
    "\\underset{x}{\\text{min }} E(x) \\text{ subject to } ||x||_2^2 = 1 \\text{ and } 1_d^T x = 0. \\\\\n",
    "E(x) = \\sum\\limits_{ij}w_{ij}(x_i-x_j)^2 = \\sum\\limits_{ij} w_{ij}(x_i^2-2x_ix_j+x_j^2)\n",
    "= \\sum\\limits_{i} a_i x_i^2 -2 \\sum\\limits_{ij}w_{ij}x_ix_j + \\sum\\limits_{j} a_j x_j^2\n",
    "= 2 x^T (A-W) x\n",
    "$\n",
    "\n",
    "where $W$ is the matrix of similarity scores, $a = W 1_d = W^T 1_d$ and $A=diag(a)$.\n",
    "\n",
    "So the task is minimizing $x^T (A-W) x$ subject to $||x||_2^2 = 1$ and $1_d^T x = 0$. Note that $A-W$ is a symmetric matrix. Indeed it is also positive semi-definite since it is diagonally dominant matrix (https://en.wikipedia.org/wiki/Diagonally_dominant_matrix#Applications_and_properties) (A Hermitian diagonally dominant matrix with real non-negative diagonal entries is positive semidefinite.). Since it is a symmetric positive semi-definite matrix, its all eigenvalues are real and nonnegative. Indeed $0$ is the smallest eigenvalue of $A-W$ with eigenvector $1_d$:\n",
    "\n",
    "$\n",
    "(A-W) 1_d = A 1_d - W 1_d = a - a = 0_d = 0 \\cdot 1_d\n",
    "$\n",
    "\n",
    "However, this eigenvector violates the constraints. So we choose the eigenvector with the second smallest eigenvalue as the solution to the problem. The dimensionality of the manifold can be increased by simply going to the other small eigenvectors in order.\n",
    "\n",
    "__Proof:__ $C(x) = 2 x^T (A-W) x - \\lambda_1 (1-x^Tx) - \\lambda_2(1_d^T x)\n",
    "\\\\ \\nabla_x C(x) = 4(A-W)x + 2 \\lambda_1 x - \\lambda_2 1_d = 0\n",
    "$\n",
    "\n",
    "Multiplying both sides of equality with $1_d^T$ from left, we reach\n",
    "\n",
    "$\n",
    "4 \\cdot \\underbrace{1_d^T(A-W)}_{0}x + 2 \\lambda_1 \\underbrace{1_d^Tx}_{0} - d \\cdot \\lambda_2 = 0 \\text{ so, } \\lambda_2 = 0\n",
    "$\n",
    "\n",
    "Therefore $2(W-A)x = \\lambda_1 x$. We don't accept $\\lambda_1 = 0$ which and corresponding eigenvector $1_d$. Instead, we take the second smallest eigenvalue's corresponding eigenvector as the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0-rc1",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
